{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "55d26d79",
      "metadata": {
        "id": "55d26d79"
      },
      "source": [
        "# NLP-Based Multiple Choice Question Generator\n",
        "\n",
        "## Project Overview\n",
        "This project generates multiple choice questions (MCQs) from PDF documents using:\n",
        "- **Natural Language Processing (NLP)** techniques: tokenization, stemming, lemmatization\n",
        "- **Google Gemini API** for intelligent question generation\n",
        "- **Text analysis** features for keyword extraction and concept identification\n",
        "\n",
        "### Key NLP Features Used:\n",
        "1. **Tokenization**: Breaking text into sentences and words\n",
        "2. **Stemming**: Reducing words to their root form using Porter Stemmer\n",
        "3. **Lemmatization**: Converting words to their base/dictionary form\n",
        "4. **POS Tagging**: Identifying parts of speech\n",
        "5. **Named Entity Recognition**: Identifying important entities\n",
        "6. **TF-IDF**: Extracting important keywords and concepts\n",
        "7. **Stop Word Removal**: Filtering out common words\n",
        "\n",
        "### Subject: Natural Language Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef673d8f",
      "metadata": {
        "id": "ef673d8f"
      },
      "source": [
        "## Step 1: Install Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2139a411",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2139a411",
        "outputId": "1804c39c-53d5-48c1-91f6-9630bc004ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.182.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.4)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.4)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "All dependencies installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install PyPDF2 google-generativeai nltk spacy scikit-learn\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ac46f21",
      "metadata": {
        "id": "6ac46f21"
      },
      "source": [
        "## Step 2: Import Libraries and MCQ Generator Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28efe751",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28efe751",
        "outputId": "17f916ea-6354-4264-d1ee-52bad23c1f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import warnings\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from collections import Counter\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "from io import BytesIO\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.tag import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import files\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN5Fj2onky2n",
        "outputId": "3171333b-477b-4505-a3c8-493e07558f6b"
      },
      "id": "FN5Fj2onky2n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff42913",
      "metadata": {
        "id": "aff42913"
      },
      "source": [
        "## Step 3: MCQ Generator Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb32e7d",
      "metadata": {
        "id": "cfb32e7d"
      },
      "outputs": [],
      "source": [
        "class MCQGenerator:\n",
        "    \"\"\"\n",
        "    A comprehensive Multiple Choice Question Generator using NLP and GenAI\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gemini_api_key: str):\n",
        "        \"\"\"\n",
        "        Initialize the MCQ Generator with Gemini API key\n",
        "\n",
        "        Args:\n",
        "            gemini_api_key (str): Google Gemini API key\n",
        "        \"\"\"\n",
        "        # Initialize Gemini API\n",
        "        genai.configure(api_key=gemini_api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "        # Initialize NLP tools\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        # Download required NLTK data\n",
        "        self._download_nltk_data()\n",
        "\n",
        "        # Load spaCy model\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except OSError:\n",
        "            print(\"SpaCy model not found. Please install with: python -m spacy download en_core_web_sm\")\n",
        "            self.nlp = None\n",
        "\n",
        "        # Initialize stop words\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        print(\"MCQ Generator initialized successfully!\")\n",
        "\n",
        "    def _download_nltk_data(self):\n",
        "        \"\"\"Download required NLTK data\"\"\"\n",
        "        nltk_downloads = [\n",
        "            'punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger',\n",
        "            'maxent_ne_chunker', 'words', 'omw-1.4'\n",
        "        ]\n",
        "\n",
        "        for item in nltk_downloads:\n",
        "            try:\n",
        "                nltk.download(item, quiet=True)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_file) -> str:\n",
        "        \"\"\"\n",
        "        Extract text from PDF file\n",
        "\n",
        "        Args:\n",
        "            pdf_file: PDF file object or path\n",
        "\n",
        "        Returns:\n",
        "            str: Extracted text from PDF\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if isinstance(pdf_file, str):\n",
        "                # If it's a file path\n",
        "                with open(pdf_file, 'rb') as file:\n",
        "                    pdf_reader = PyPDF2.PdfReader(file)\n",
        "                    text = \"\"\n",
        "                    for page in pdf_reader.pages:\n",
        "                        text += page.extract_text() + \"\\n\"\n",
        "            else:\n",
        "                # If it's a file object (for Colab uploads)\n",
        "                pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "\n",
        "            print(f\"Successfully extracted {len(text)} characters from PDF\")\n",
        "            return text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from PDF: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def preprocess_text(self, text: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Comprehensive text preprocessing using NLP techniques\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "\n",
        "        Returns:\n",
        "            Dict: Preprocessed text data\n",
        "        \"\"\"\n",
        "        # Clean text\n",
        "        clean_text = re.sub(r'[^\\w\\s\\.\\?\\!]', ' ', text)\n",
        "        clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "\n",
        "        # Tokenization\n",
        "        sentences = sent_tokenize(clean_text)\n",
        "        words = word_tokenize(clean_text.lower())\n",
        "\n",
        "        # Remove stop words\n",
        "        words_no_stop = [word for word in words if word not in self.stop_words and len(word) > 2]\n",
        "\n",
        "        # Stemming\n",
        "        stemmed_words = [self.stemmer.stem(word) for word in words_no_stop]\n",
        "\n",
        "        # Lemmatization\n",
        "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words_no_stop]\n",
        "\n",
        "        # POS tagging\n",
        "        pos_tags = pos_tag(words_no_stop)\n",
        "\n",
        "        # Named Entity Recognition (if spaCy is available)\n",
        "        named_entities = []\n",
        "        if self.nlp:\n",
        "            doc = self.nlp(clean_text)\n",
        "            named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "        # Extract nouns and verbs (important concepts)\n",
        "        important_words = [word for word, pos in pos_tags if pos.startswith(('NN', 'VB'))]\n",
        "\n",
        "        preprocessed_data = {\n",
        "            'original_text': text,\n",
        "            'clean_text': clean_text,\n",
        "            'sentences': sentences,\n",
        "            'words': words,\n",
        "            'words_no_stop': words_no_stop,\n",
        "            'stemmed_words': stemmed_words,\n",
        "            'lemmatized_words': lemmatized_words,\n",
        "            'pos_tags': pos_tags,\n",
        "            'named_entities': named_entities,\n",
        "            'important_words': important_words\n",
        "        }\n",
        "\n",
        "        print(f\"Text preprocessing completed:\")\n",
        "        print(f\"- Sentences: {len(sentences)}\")\n",
        "        print(f\"- Words (no stop words): {len(words_no_stop)}\")\n",
        "        print(f\"- Named entities: {len(named_entities)}\")\n",
        "\n",
        "        return preprocessed_data\n",
        "\n",
        "    def extract_keywords(self, preprocessed_data: Dict, top_k: int = 20) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract important keywords using TF-IDF\n",
        "\n",
        "        Args:\n",
        "            preprocessed_data (Dict): Preprocessed text data\n",
        "            top_k (int): Number of top keywords to return\n",
        "\n",
        "        Returns:\n",
        "            List[str]: List of important keywords\n",
        "        \"\"\"\n",
        "        sentences = preprocessed_data['sentences']\n",
        "\n",
        "        # Use TF-IDF to find important terms\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            max_features=1000,\n",
        "            ngram_range=(1, 2),\n",
        "            stop_words='english'\n",
        "        )\n",
        "\n",
        "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        # Get average TF-IDF scores\n",
        "        mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)\n",
        "\n",
        "        # Get top keywords\n",
        "        top_indices = mean_scores.argsort()[-top_k:][::-1]\n",
        "        keywords = [feature_names[i] for i in top_indices]\n",
        "\n",
        "        print(f\"Extracted {len(keywords)} keywords: {keywords[:10]}...\")\n",
        "        return keywords\n",
        "\n",
        "    def identify_key_concepts(self, preprocessed_data: Dict) -> List[str]:\n",
        "        \"\"\"\n",
        "        Identify key concepts from the text\n",
        "\n",
        "        Args:\n",
        "            preprocessed_data (Dict): Preprocessed text data\n",
        "\n",
        "        Returns:\n",
        "            List[str]: List of key concepts\n",
        "        \"\"\"\n",
        "        concepts = []\n",
        "\n",
        "        # Add named entities\n",
        "        entities = [entity[0] for entity in preprocessed_data['named_entities']]\n",
        "        concepts.extend(entities)\n",
        "\n",
        "        # Add frequent important words\n",
        "        important_words = preprocessed_data['important_words']\n",
        "        word_freq = Counter(important_words)\n",
        "        frequent_concepts = [word for word, freq in word_freq.most_common(15)]\n",
        "        concepts.extend(frequent_concepts)\n",
        "\n",
        "        # Remove duplicates and filter\n",
        "        concepts = list(set(concepts))\n",
        "        concepts = [concept for concept in concepts if len(concept) > 3]\n",
        "\n",
        "        print(f\"Identified {len(concepts)} key concepts\")\n",
        "        return concepts\n",
        "\n",
        "    def generate_mcq_with_gemini(self, text_chunk: str, num_questions: int = 1) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Generate MCQ questions using Google Gemini\n",
        "\n",
        "        Args:\n",
        "            text_chunk (str): Text chunk to generate questions from\n",
        "            num_questions (int): Number of questions to generate\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: List of generated MCQ questions\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Based on the following text, generate {num_questions} multiple choice question(s) with 4 options each.\n",
        "        The questions should test comprehension and understanding of key concepts.\n",
        "\n",
        "        Format the response as a JSON array where each question has:\n",
        "        - \"question\": the question text\n",
        "        - \"options\": array of 4 options (A, B, C, D)\n",
        "        - \"correct_answer\": the correct option letter (A, B, C, or D)\n",
        "        - \"explanation\": brief explanation of why the answer is correct\n",
        "\n",
        "        Text:\n",
        "        {text_chunk}\n",
        "\n",
        "        Provide only the JSON array response, no additional text.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            response_text = response.text\n",
        "\n",
        "            # Clean the response to extract JSON\n",
        "            response_text = response_text.strip()\n",
        "            if response_text.startswith('```json'):\n",
        "                response_text = response_text[7:]\n",
        "            if response_text.endswith('```'):\n",
        "                response_text = response_text[:-3]\n",
        "\n",
        "            questions = json.loads(response_text)\n",
        "            return questions if isinstance(questions, list) else [questions]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating questions with Gemini: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def select_best_chunks(self, sentences: List[str], chunk_size: int = 3, num_chunks: int = 5) -> List[str]:\n",
        "        \"\"\"\n",
        "        Select the best text chunks for question generation\n",
        "\n",
        "        Args:\n",
        "            sentences (List[str]): List of sentences\n",
        "            chunk_size (int): Size of each chunk in sentences\n",
        "            num_chunks (int): Number of chunks to select\n",
        "\n",
        "        Returns:\n",
        "            List[str]: Selected text chunks\n",
        "        \"\"\"\n",
        "        # Create chunks\n",
        "        chunks = []\n",
        "        for i in range(0, len(sentences) - chunk_size + 1, chunk_size // 2):\n",
        "            chunk = ' '.join(sentences[i:i + chunk_size])\n",
        "            if len(chunk.strip()) > 100:  # Minimum chunk length\n",
        "                chunks.append(chunk)\n",
        "\n",
        "        if len(chunks) <= num_chunks:\n",
        "            return chunks\n",
        "\n",
        "        # Use TF-IDF to score chunks\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "\n",
        "        # Calculate chunk importance (sum of TF-IDF scores)\n",
        "        chunk_scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
        "\n",
        "        # Select top chunks\n",
        "        top_indices = chunk_scores.argsort()[-num_chunks:][::-1]\n",
        "        selected_chunks = [chunks[i] for i in top_indices]\n",
        "\n",
        "        print(f\"Selected {len(selected_chunks)} chunks for question generation\")\n",
        "        return selected_chunks\n",
        "\n",
        "    def generate_mcq_questions(self, pdf_file, num_questions: int = 10) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Main function to generate MCQ questions from PDF\n",
        "\n",
        "        Args:\n",
        "            pdf_file: PDF file object or path\n",
        "            num_questions (int): Number of questions to generate\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: Generated MCQ questions\n",
        "        \"\"\"\n",
        "        print(\"Starting MCQ generation process...\")\n",
        "\n",
        "        # Step 1: Extract text from PDF\n",
        "        text = self.extract_text_from_pdf(pdf_file)\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        # Step 2: Preprocess text\n",
        "        preprocessed_data = self.preprocess_text(text)\n",
        "\n",
        "        # Step 3: Extract keywords and concepts\n",
        "        keywords = self.extract_keywords(preprocessed_data)\n",
        "        concepts = self.identify_key_concepts(preprocessed_data)\n",
        "\n",
        "        # Step 4: Select best text chunks\n",
        "        sentences = preprocessed_data['sentences']\n",
        "        selected_chunks = self.select_best_chunks(sentences, num_chunks=min(num_questions, 8))\n",
        "\n",
        "        # Step 5: Generate questions\n",
        "        all_questions = []\n",
        "        questions_per_chunk = max(1, num_questions // len(selected_chunks))\n",
        "\n",
        "        for i, chunk in enumerate(selected_chunks):\n",
        "            if len(all_questions) >= num_questions:\n",
        "                break\n",
        "\n",
        "            questions_needed = min(questions_per_chunk, num_questions - len(all_questions))\n",
        "            chunk_questions = self.generate_mcq_with_gemini(chunk, questions_needed)\n",
        "\n",
        "            for question in chunk_questions:\n",
        "                question['source_chunk'] = f\"Chunk {i+1}\"\n",
        "                question['keywords'] = keywords[:5]  # Add relevant keywords\n",
        "\n",
        "            all_questions.extend(chunk_questions)\n",
        "            print(f\"Generated {len(chunk_questions)} questions from chunk {i+1}\")\n",
        "\n",
        "        # Shuffle questions\n",
        "        random.shuffle(all_questions)\n",
        "\n",
        "        print(f\"Successfully generated {len(all_questions)} MCQ questions!\")\n",
        "        return all_questions[:num_questions]\n",
        "\n",
        "    def format_questions_for_display(self, questions: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        Format questions for display\n",
        "\n",
        "        Args:\n",
        "            questions (List[Dict]): List of questions\n",
        "\n",
        "        Returns:\n",
        "            str: Formatted questions\n",
        "        \"\"\"\n",
        "        formatted_output = \"=\"*60 + \"\\n\"\n",
        "        formatted_output += \"MULTIPLE CHOICE QUESTIONS\\n\"\n",
        "        formatted_output += \"=\"*60 + \"\\n\\n\"\n",
        "\n",
        "        for i, q in enumerate(questions, 1):\n",
        "            formatted_output += f\"Question {i}:\\n\"\n",
        "            formatted_output += f\"{q['question']}\\n\\n\"\n",
        "\n",
        "            for j, option in enumerate(q['options']):\n",
        "                letter = chr(65 + j)  # A, B, C, D\n",
        "                formatted_output += f\"{letter}) {option}\\n\"\n",
        "\n",
        "            formatted_output += f\"\\nCorrect Answer: {q['correct_answer']}\\n\"\n",
        "            formatted_output += f\"Explanation: {q['explanation']}\\n\"\n",
        "\n",
        "            if 'keywords' in q:\n",
        "                formatted_output += f\"Related Keywords: {', '.join(q['keywords'])}\\n\"\n",
        "\n",
        "            formatted_output += \"\\n\" + \"-\"*40 + \"\\n\\n\"\n",
        "\n",
        "        return formatted_output\n",
        "\n",
        "    def demonstrate_nlp_features(self, text: str):\n",
        "        \"\"\"\n",
        "        Demonstrate NLP features used in the project\n",
        "\n",
        "        Args:\n",
        "            text (str): Sample text to analyze\n",
        "        \"\"\"\n",
        "        print(\"=== NLP FEATURES DEMONSTRATION ===\")\n",
        "        print(f\"Original text: {text[:200]}...\\n\")\n",
        "\n",
        "        # Tokenization\n",
        "        sentences = sent_tokenize(text)\n",
        "        words = word_tokenize(text.lower())\n",
        "        print(f\"1. TOKENIZATION:\")\n",
        "        print(f\"   - Sentences: {len(sentences)}\")\n",
        "        print(f\"   - Words: {len(words)}\")\n",
        "        print(f\"   - First 10 words: {words[:10]}\\n\")\n",
        "\n",
        "        # Stop word removal\n",
        "        words_no_stop = [word for word in words if word not in self.stop_words and len(word) > 2]\n",
        "        print(f\"2. STOP WORD REMOVAL:\")\n",
        "        print(f\"   - Words after removal: {len(words_no_stop)}\")\n",
        "        print(f\"   - Sample: {words_no_stop[:10]}\\n\")\n",
        "\n",
        "        # Stemming\n",
        "        stemmed_words = [self.stemmer.stem(word) for word in words_no_stop[:10]]\n",
        "        print(f\"3. STEMMING (Porter Stemmer):\")\n",
        "        print(f\"   - Original: {words_no_stop[:10]}\")\n",
        "        print(f\"   - Stemmed:  {stemmed_words}\\n\")\n",
        "\n",
        "        # Lemmatization\n",
        "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words_no_stop[:10]]\n",
        "        print(f\"4. LEMMATIZATION:\")\n",
        "        print(f\"   - Original:     {words_no_stop[:10]}\")\n",
        "        print(f\"   - Lemmatized:   {lemmatized_words}\\n\")\n",
        "\n",
        "        # POS Tagging\n",
        "        pos_tags = pos_tag(words_no_stop[:10])\n",
        "        print(f\"5. POS TAGGING:\")\n",
        "        print(f\"   - Tags: {pos_tags}\\n\")\n",
        "\n",
        "        # Named Entity Recognition\n",
        "        if self.nlp:\n",
        "            doc = self.nlp(text[:500])  # Limit text for demo\n",
        "            entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "            print(f\"6. NAMED ENTITY RECOGNITION:\")\n",
        "            print(f\"   - Entities found: {entities[:10]}\\n\")\n",
        "\n",
        "        # TF-IDF Keywords\n",
        "        preprocessed_data = self.preprocess_text(text)\n",
        "        keywords = self.extract_keywords(preprocessed_data, top_k=10)\n",
        "        print(f\"7. TF-IDF KEYWORD EXTRACTION:\")\n",
        "        print(f\"   - Top keywords: {keywords}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc08c2ca",
      "metadata": {
        "id": "fc08c2ca"
      },
      "source": [
        "## Step 4: Configuration and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1dff9c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1dff9c0",
        "outputId": "3a9a375b-0935-4a58-cd4a-fea493e46d23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCQ Generator initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "GEMINI_API_KEY = \"x\"\n",
        "\n",
        "# Initialize the MCQ Generator\n",
        "if GEMINI_API_KEY == \"YOUR_GEMINI_API_KEY_HERE\":\n",
        "    print(\"WARNING: Please replace GEMINI_API_KEY with your actual API key!\")\n",
        "else:\n",
        "    generator = MCQGenerator(GEMINI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43b32520",
      "metadata": {
        "id": "43b32520"
      },
      "source": [
        "## Step 5: Upload and Process PDF File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b47b0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "c1b47b0d",
        "outputId": "a181bb46-f798-452a-cfca-d492a91aa7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload a PDF file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f7d3b954-2c43-4597-ac39-9343c08c0e14\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f7d3b954-2c43-4597-ac39-9343c08c0e14\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2.2_Map Reduce.pdf to 2.2_Map Reduce.pdf\n",
            "Uploaded file: 2.2_Map Reduce.pdf\n",
            "File ready for processing!\n"
          ]
        }
      ],
      "source": [
        "# Upload PDF file\n",
        "print(\"Please upload a PDF file:\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Get the uploaded file\n",
        "if uploaded_files:\n",
        "    filename = list(uploaded_files.keys())[0]\n",
        "    print(f\"Uploaded file: {filename}\")\n",
        "\n",
        "    # Create file object for processing\n",
        "    pdf_content = uploaded_files[filename]\n",
        "    pdf_file = BytesIO(pdf_content)\n",
        "\n",
        "    print(f\"File ready for processing!\")\n",
        "else:\n",
        "    print(\"No file uploaded. Please upload a PDF to proceed.\")\n",
        "    pdf_file = None # Ensure pdf_file is None if no file uploaded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95daa6c9",
      "metadata": {
        "id": "95daa6c9"
      },
      "source": [
        "## Step 7: Generate MCQ Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e3f2d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "93e3f2d5",
        "outputId": "bf76393b-8678-48c0-bf6b-1877e5da605b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 5 MCQ questions from uploaded PDF...\n",
            "Starting MCQ generation process...\n",
            "Successfully extracted 3320 characters from PDF\n",
            "Text preprocessing completed:\n",
            "- Sentences: 31\n",
            "- Words (no stop words): 318\n",
            "- Named entities: 35\n",
            "Extracted 20 keywords: ['mapreduce', 'data', 'map', 'stage', 'shuffle', 'phase', 'mapper', 'output', 'reduce', 'key']...\n",
            "Identified 31 key concepts\n",
            "Selected 5 chunks for question generation\n",
            "Generated 1 questions from chunk 1\n",
            "Error generating questions with Gemini: Expecting value: line 10 column 5 (char 367)\n",
            "Generated 0 questions from chunk 2\n",
            "Generated 1 questions from chunk 3\n",
            "Generated 1 questions from chunk 4\n",
            "Generated 1 questions from chunk 5\n",
            "Successfully generated 4 MCQ questions!\n",
            "\n",
            "Successfully generated 4 questions!\n"
          ]
        }
      ],
      "source": [
        "# Set parameters\n",
        "NUM_QUESTIONS = 5  # Adjust as needed\n",
        "\n",
        "if GEMINI_API_KEY != \"YOUR_GEMINI_API_KEY_HERE\":\n",
        "    if pdf_file:\n",
        "        # Generate questions from uploaded PDF\n",
        "        print(f\"Generating {NUM_QUESTIONS} MCQ questions from uploaded PDF...\")\n",
        "        questions = generator.generate_mcq_questions(pdf_file, num_questions=NUM_QUESTIONS)\n",
        "    else:\n",
        "        # Handle the case where no file was uploaded\n",
        "        print(\"No PDF file uploaded. Please upload a PDF in the previous step.\")\n",
        "        questions = [] # Ensure questions is empty if no file\n",
        "\n",
        "    if questions:\n",
        "        print(f\"\\nSuccessfully generated {len(questions)} questions!\")\n",
        "    else:\n",
        "        print(\"Failed to generate questions or no PDF was uploaded. Check your API key and try again.\")\n",
        "else:\n",
        "    print(\"Please set your Gemini API key first!\")\n",
        "    questions = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33885cf3",
      "metadata": {
        "id": "33885cf3"
      },
      "source": [
        "## Step 8: Display Generated Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0eceec0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0eceec0",
        "outputId": "e75bfa51-72ff-46bf-a402-5b1a6a9eba03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MULTIPLE CHOICE QUESTIONS\n",
            "============================================================\n",
            "\n",
            "Question 1:\n",
            "According to the text, what is a fundamental principle upon which the MapReduce paradigm is generally based?\n",
            "\n",
            "A) A. Sending the data to where the computer program resides.\n",
            "B) B. The reducer performs a defined function on a single value for each unique key.\n",
            "C) C. Sending the computer program to where the data resides.\n",
            "D) D. The final output key-value will only be displayed and not stored.\n",
            "\n",
            "Correct Answer: C\n",
            "Explanation: The text explicitly states: 'The Algorithm Generally MapReduce paradigm is based on sending the computer program to where the data resides!'\n",
            "Related Keywords: mapreduce, data, map, stage, shuffle\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Question 2:\n",
            "Which of the following describes the details of data passing managed by the Hadoop framework during a MapReduce job?\n",
            "\n",
            "A) A) Storing the processed output in HDFS and producing new output.\n",
            "B) B) Sending Map and Reduce tasks to servers and storing output in HDFS.\n",
            "C) C) Issuing tasks, verifying task completion, and copying data around the cluster between nodes.\n",
            "D) D) Producing new output and sending Map/Reduce tasks to appropriate servers.\n",
            "\n",
            "Correct Answer: C\n",
            "Explanation: The text explicitly states: 'The framework manages all the details of data passing such as issuing tasks verifying task completion and copying data around the cluster between the nodes.'\n",
            "Related Keywords: mapreduce, data, map, stage, shuffle\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Question 3:\n",
            "According to the text, what is a key benefit of having computing take place on nodes with data on local disks in a Hadoop MapReduce job?\n",
            "\n",
            "A) A. It allows for faster overall job completion by utilizing SSDs.\n",
            "B) B. It reduces the need for data replication across the cluster.\n",
            "C) C. It significantly decreases the amount of network traffic.\n",
            "D) D. It simplifies the programming model for MapReduce tasks.\n",
            "\n",
            "Correct Answer: C\n",
            "Explanation: The text states: 'Most of the computing take s place on nodes with data on local disks that reduces the network traffic.' This directly links computing on local disks to a reduction in network traffic.\n",
            "Related Keywords: mapreduce, data, map, stage, shuffle\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Question 4:\n",
            "According to the text, what is the primary purpose of MapReduce?\n",
            "\n",
            "A) A) To develop single-threaded applications for data entry.\n",
            "B) B) To process huge amounts of data in parallel on large clusters of commodity hardware.\n",
            "C) C) To design user interfaces for Java-based applications.\n",
            "D) D) To manage network security protocols in a distributed system.\n",
            "\n",
            "Correct Answer: B\n",
            "Explanation: The text explicitly states: 'MapReduce is a framework using which we can write applications to process huge amounts of data in parallel on large clusters of commodity hardware in a reliable manner.'\n",
            "Related Keywords: mapreduce, data, map, stage, shuffle\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "============================================================\n",
            "STRUCTURED QUESTION DATA (JSON Format)\n",
            "============================================================\n",
            "[\n",
            "  {\n",
            "    \"question\": \"According to the text, what is a fundamental principle upon which the MapReduce paradigm is generally based?\",\n",
            "    \"options\": [\n",
            "      \"A. Sending the data to where the computer program resides.\",\n",
            "      \"B. The reducer performs a defined function on a single value for each unique key.\",\n",
            "      \"C. Sending the computer program to where the data resides.\",\n",
            "      \"D. The final output key-value will only be displayed and not stored.\"\n",
            "    ],\n",
            "    \"correct_answer\": \"C\",\n",
            "    \"explanation\": \"The text explicitly states: 'The Algorithm Generally MapReduce paradigm is based on sending the computer program to where the data resides!'\",\n",
            "    \"source_chunk\": \"Chunk 4\",\n",
            "    \"keywords\": [\n",
            "      \"mapreduce\",\n",
            "      \"data\",\n",
            "      \"map\",\n",
            "      \"stage\",\n",
            "      \"shuffle\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"Which of the following describes the details of data passing managed by the Hadoop framework during a MapReduce job?\",\n",
            "    \"options\": [\n",
            "      \"A) Storing the processed output in HDFS and producing new output.\",\n",
            "      \"B) Sending Map and Reduce tasks to servers and storing output in HDFS.\",\n",
            "      \"C) Issuing tasks, verifying task completion, and copying data around the cluster between nodes.\",\n",
            "      \"D) Producing new output and sending Map/Reduce tasks to appropriate servers.\"\n",
            "    ],\n",
            "    \"correct_answer\": \"C\",\n",
            "    \"explanation\": \"The text explicitly states: 'The framework manages all the details of data passing such as issuing tasks verifying task completion and copying data around the cluster between the nodes.'\",\n",
            "    \"source_chunk\": \"Chunk 3\",\n",
            "    \"keywords\": [\n",
            "      \"mapreduce\",\n",
            "      \"data\",\n",
            "      \"map\",\n",
            "      \"stage\",\n",
            "      \"shuffle\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"According to the text, what is a key benefit of having computing take place on nodes with data on local disks in a Hadoop MapReduce job?\",\n",
            "    \"options\": [\n",
            "      \"A. It allows for faster overall job completion by utilizing SSDs.\",\n",
            "      \"B. It reduces the need for data replication across the cluster.\",\n",
            "      \"C. It significantly decreases the amount of network traffic.\",\n",
            "      \"D. It simplifies the programming model for MapReduce tasks.\"\n",
            "    ],\n",
            "    \"correct_answer\": \"C\",\n",
            "    \"explanation\": \"The text states: 'Most of the computing take s place on nodes with data on local disks that reduces the network traffic.' This directly links computing on local disks to a reduction in network traffic.\",\n",
            "    \"source_chunk\": \"Chunk 5\",\n",
            "    \"keywords\": [\n",
            "      \"mapreduce\",\n",
            "      \"data\",\n",
            "      \"map\",\n",
            "      \"stage\",\n",
            "      \"shuffle\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"According to the text, what is the primary purpose of MapReduce?\",\n",
            "    \"options\": [\n",
            "      \"A) To develop single-threaded applications for data entry.\",\n",
            "      \"B) To process huge amounts of data in parallel on large clusters of commodity hardware.\",\n",
            "      \"C) To design user interfaces for Java-based applications.\",\n",
            "      \"D) To manage network security protocols in a distributed system.\"\n",
            "    ],\n",
            "    \"correct_answer\": \"B\",\n",
            "    \"explanation\": \"The text explicitly states: 'MapReduce is a framework using which we can write applications to process huge amounts of data in parallel on large clusters of commodity hardware in a reliable manner.'\",\n",
            "    \"source_chunk\": \"Chunk 1\",\n",
            "    \"keywords\": [\n",
            "      \"mapreduce\",\n",
            "      \"data\",\n",
            "      \"map\",\n",
            "      \"stage\",\n",
            "      \"shuffle\"\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Display the generated questions\n",
        "if questions:\n",
        "    formatted_questions = generator.format_questions_for_display(questions)\n",
        "    print(formatted_questions)\n",
        "\n",
        "    # Also display as structured data\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STRUCTURED QUESTION DATA (JSON Format)\")\n",
        "    print(\"=\"*60)\n",
        "    print(json.dumps(questions, indent=2, ensure_ascii=False))\n",
        "else:\n",
        "    print(\"No questions to display. Please check the previous steps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810784d4",
      "metadata": {
        "id": "810784d4"
      },
      "source": [
        "## Step 9: Analysis and Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adadfdd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adadfdd4",
        "outputId": "d42344a1-df16-4662-ef76-31b51c4f29ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== QUESTION ANALYSIS ===\n",
            "Total questions generated: 4\n",
            "Average question length: 18.2 words\n",
            "Question length range: 11 - 26 words\n",
            "\n",
            "Answer distribution: {'C': 3, 'B': 1}\n",
            "\n",
            "Most common keywords: [('mapreduce', 4), ('data', 4), ('map', 4), ('stage', 4), ('shuffle', 4)]\n",
            "\n",
            "✅ Analysis complete!\n"
          ]
        }
      ],
      "source": [
        "# Analyze the generated questions\n",
        "if questions:\n",
        "    print(\"=== QUESTION ANALYSIS ===\")\n",
        "    print(f\"Total questions generated: {len(questions)}\")\n",
        "\n",
        "    # Analyze question lengths\n",
        "    question_lengths = [len(q['question'].split()) for q in questions]\n",
        "    print(f\"Average question length: {np.mean(question_lengths):.1f} words\")\n",
        "    print(f\"Question length range: {min(question_lengths)} - {max(question_lengths)} words\")\n",
        "\n",
        "    # Analyze answer distribution\n",
        "    answer_distribution = Counter([q['correct_answer'] for q in questions])\n",
        "    print(f\"\\nAnswer distribution: {dict(answer_distribution)}\")\n",
        "\n",
        "    # Extract all keywords used\n",
        "    all_keywords = []\n",
        "    for q in questions:\n",
        "        if 'keywords' in q:\n",
        "            all_keywords.extend(q['keywords'])\n",
        "\n",
        "    if all_keywords:\n",
        "        keyword_freq = Counter(all_keywords)\n",
        "        print(f\"\\nMost common keywords: {keyword_freq.most_common(10)}\")\n",
        "\n",
        "    print(f\"\\n✅ Analysis complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b197559",
      "metadata": {
        "id": "0b197559"
      },
      "source": [
        "## Project Summary\n",
        "\n",
        "### NLP Techniques Used:\n",
        "1. **Tokenization**: Breaking text into sentences and words using NLTK\n",
        "2. **Stemming**: Using Porter Stemmer to reduce words to root forms\n",
        "3. **Lemmatization**: Converting words to dictionary base forms using WordNet\n",
        "4. **Stop Word Removal**: Filtering common words for better analysis\n",
        "5. **POS Tagging**: Identifying parts of speech for important word extraction\n",
        "6. **Named Entity Recognition**: Using spaCy to identify persons, organizations, locations\n",
        "7. **TF-IDF Vectorization**: Extracting important keywords and ranking text chunks\n",
        "8. **Text Preprocessing**: Cleaning and normalizing text data\n",
        "\n",
        "### GenAI Integration:\n",
        "- **Google Gemini API**: Generating intelligent, contextual MCQ questions\n",
        "- **Prompt Engineering**: Structured prompts for consistent question format\n",
        "\n",
        "### Key Features:\n",
        "- PDF text extraction and processing\n",
        "- Intelligent text chunk selection for question generation\n",
        "- Comprehensive NLP analysis and preprocessing\n",
        "- Automated MCQ generation with explanations\n",
        "- Statistical analysis of generated questions\n",
        "- Export functionality for different formats\n",
        "\n",
        "### Academic Value:\n",
        "This project demonstrates practical application of NLP concepts including:\n",
        "- Text preprocessing pipelines\n",
        "- Feature extraction techniques\n",
        "- Information retrieval methods\n",
        "- Integration of traditional NLP with modern GenAI\n",
        "- Real-world application development\n",
        "\n",
        "Perfect for NLP course projects and demonstrates comprehensive understanding of both classical NLP techniques and modern AI applications!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
